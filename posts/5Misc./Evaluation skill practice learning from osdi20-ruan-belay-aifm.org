#+TITLE: Evaluation Skill Practice: Learning from OSDI'20-Ruan-AIFM
#+DATE: Nov 20, 2020
#+OPTIONS: num:0

Recently an OSDI'20 paper, [[https://www.usenix.org/conference/osdi20/presentation/ruan][AIFM: High-Performance,
Application-Integrated Far Memory]], has aroused heated discussions
in our lab.  It is interesting, and relevant enough that I decide
to make use of this paper to practice my evaluation skill.

The practice consists of two steps:
- If AIFM was my work, how would I evaluate it?
- Compare my evaluation plan with this OSDI paper.

* My Evaluation Plan

1. Engineering effort to use their APIs for different workloads.
2. Application performance under different level of local memory pressure.
   - Workloads: graph computing (ligra), in-memory database (Memcached), general data processing (spark)
   - Variable: 20%, 40%, 60%, 80%, 100% DRAM
   - Result: computing time
   - Group: traditional swapping, Related work (Fastswap), AIFM
3. CDF to reveal the trade-off between "slowing-down each access
   a little bit" and "largely slowing-down a fraction of accesses
   by page fault".
4. Breakdown and evaluate the effectiveness of each optimization.
   - Workload: make a micro-benchmark
   - Variable: accumulate each optimization
   - Result: time of each operation

* Their Evaluation

1. End-to-end performance
   1. *Web service*.  They designed a synthetic web service that
      accesses "remoteable" hashtable and array, and does some
      computation.
      1. Figure 1: Given a memory cap, compare the latency-against-throughput of Fastswap, AIFM-NT, AIFM-T
      2. Figure 2: varies memory cap, compare the normalized throughput of Fastswap, AIFM-NT, AIFM-T
      3. Figure 3: (supporting data of figure 2) varies memory cap, show the "remoteable" data structure miss rate of AIFM
   2. *Data frame application*.  They choose a Pandas-like C++
      data analysis framework, and a workload of NYC taxi trip
      data analysis.
      1. Figure 1: varies memory cap, compare the normalized throughput of Fastswap, AIFM, AIFM-offload
      2. Figure 2: (supporting data of figure 1) varies memory cap, show the normalized throughput of AIFM with different level of offload
2. Data structure
   1. *Hashtable*
      1. Figure 1: varies Zipfian skew parameter, compare the GET throughput of all-in-local-memory, Fastswap, AIFM
      2. Figure 2: AIFM hashtable throughput under different miss rate, under different thread number
   2. *Array*.  A compression and decompression workload.
      1. Figure 1: compression: varies memory cap, compare the normalized performance (maybe inverse of run time?) of Fastswap, AIFM
      2. Figure 2: decompression: varies memory cap, compare the normalized performance (maybe inverse of run time?) of Fastswap, AIFM
3. Evaluating specific design aspect
   1. *Object access latency*
      1. Table 1: 90th percentile latency (in cycles) of {read, write} dereferencing normal C++ pointer and a AIFM remoteable pointer.
      2. Table 2: 90th percentile latency (in cycles) of {read, write} fetching a remote object of Fastswap, AIFM with {64B, 4KB} object size.
   2. *Balance point of compute time vs fetching time*.  They designed a micro-benchmark to do sequential read + compute over an array.
      1. Figure 1: varies compute time, compare the normalized run time of Fastswap, AIFM.
   3. *Pauseless Evacuation*
      1. Figure 1: application thread performance impact: compare stop-the-world evacuator and pauseless evacuator.
   4. *Prioritizing scheduler*
      1. Figure 1: the effectiveness of object eviction: compare with, and without their prioritizing scheduler.

* Lessons learned

1. Organize the evaluation in a logical order (typically a
   top-down order): from end-to-end application benchmarks, to
   data structure benchmarks, to the evaluations of partial
   section of the whole system.
2. Decide which parameter(s) to vary, which parameter(s) to fix,
   according to the evaluation goal.  For example, they fix
   Zipfian ratio and vary memory cap in the web service
   end-to-end evaluation; and they fix memory cap and vary
   Zipfian ratio in the hashtable evaluation.
3. Use some supporting data to better explain/illustrate the
   result especially in an end-to-end evaluation.
4. If measuring each memory access latency for a CDF plot is too
   hard, one can use statistical result (mean, median, p90, p99).
5. Evaluate partial section of the whole system (a specific
   design choice).
6. Do break-even analysis.  Show when and how the benefits of the
   system may be offset by other factors.
